{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Transformer 架构图](https://www.runoob.com/wp-content/uploads/2025/03/Transformer_full_architecture.png)",
   "id": "64a5b81df0b90290"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```\n",
    "词嵌入（✳）\n",
    "位置编码（✳）\n",
    "---\n",
    "归一化操作\n",
    "前馈神经网络（增强非线性变换）\n",
    "残差连接\n",
    "---\n",
    "注意力机制\n",
    "多头自注意力\n",
    "掩码注意力\n",
    "交叉注意力\n",
    "```"
   ],
   "id": "abd0ee4aa5019a12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T04:19:11.759778Z",
     "start_time": "2025-11-23T04:19:11.743956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "# 导入库文件\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "id": "7552fa56e4ce707a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-23T04:19:11.775670Z",
     "start_time": "2025-11-23T04:19:11.760153Z"
    }
   },
   "source": [
    "# Embedding (词向量编码)\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, d_modl):\n",
    "        super(TokenEmbedding, self).__init__(vocab_size, d_modl, padding_idx=1)\n",
    "\n",
    "# PositionalEmbedding (位置编码)\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, device):\n",
    "        super(PositionalEmbedding,self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "        self.encoding.requires_grad = False\n",
    "        pos = torch.arange(0, max_len, device=device)\n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "        self.encoding[:,0::2] = torch.sin(pos/(10000**(_2i/d_model)))\n",
    "        self.encoding[:,1::2] = torch.cos(pos/(10000**(_2i/d_model)))\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        return self.encoding[:seq_len,:]\n",
    " \n",
    " # 总编码（词嵌入+位置）   \n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
    "        super(TransformerEmbedding,self).__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEmbedding(d_model,max_len,device)\n",
    "        self.drop_out = nn.Dropout(p=drop_prob)\n",
    "    def forward(self,x):\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb(x)\n",
    "        return self.drop_out(tok_emb+pos_emb)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T04:19:11.822928Z",
     "start_time": "2025-11-23T04:19:11.775670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 多头注意力机制\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_combine = nn.Linear(d_model,d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    def forward(self, q, k, v,mask=None):\n",
    "        batch,time,dimension = q.shape\n",
    "        n_d = self.d_model//self.n_head\n",
    "        q,k,v = self.w_q(q),self.w_k(k),self.w_v(v)\n",
    "        q = q.view(batch,time,self.n_head,n_d).permute(0,2,1,3)\n",
    "        k = k.view(batch,time,self.n_head,n_d).permute(0,2,1,3)\n",
    "        v = v.view(batch,time,self.n_head,n_d).permute(0,2,1,3)\n",
    "        score = q@k.transpose(2,3)/math.sqrt(n_d)\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask==0,-1000)\n",
    "        score = self.softmax(score)@v\n",
    "        score = score.permute(0,2,1,3).contiguous().view(batch,time,dimension)\n",
    "        output = self.w_combine(score)\n",
    "        return output \n",
    "# 测试\n",
    "xi = torch.rand(128,32,512)\n",
    "attention=MultiHeadAttention(d_model=512,n_head=8)\n",
    "out = attention(xi,xi,xi)\n",
    "print(out)\n",
    "print(out.shape)"
   ],
   "id": "20df409385c1b044",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0114,  0.2448,  0.0425,  ..., -0.1600, -0.2929, -0.2216],\n",
      "         [ 0.0111,  0.2451,  0.0417,  ..., -0.1595, -0.2934, -0.2221],\n",
      "         [ 0.0115,  0.2451,  0.0424,  ..., -0.1594, -0.2940, -0.2220],\n",
      "         ...,\n",
      "         [ 0.0123,  0.2452,  0.0424,  ..., -0.1596, -0.2934, -0.2226],\n",
      "         [ 0.0119,  0.2453,  0.0412,  ..., -0.1602, -0.2935, -0.2224],\n",
      "         [ 0.0116,  0.2447,  0.0415,  ..., -0.1601, -0.2932, -0.2219]],\n",
      "\n",
      "        [[-0.0033,  0.2351,  0.0411,  ..., -0.1433, -0.2982, -0.2241],\n",
      "         [-0.0020,  0.2354,  0.0396,  ..., -0.1439, -0.2978, -0.2235],\n",
      "         [-0.0018,  0.2351,  0.0393,  ..., -0.1436, -0.2980, -0.2227],\n",
      "         ...,\n",
      "         [-0.0031,  0.2352,  0.0401,  ..., -0.1433, -0.2982, -0.2246],\n",
      "         [-0.0023,  0.2348,  0.0401,  ..., -0.1430, -0.2971, -0.2240],\n",
      "         [-0.0032,  0.2338,  0.0399,  ..., -0.1426, -0.2983, -0.2242]],\n",
      "\n",
      "        [[ 0.0116,  0.2411,  0.0261,  ..., -0.1296, -0.2872, -0.2079],\n",
      "         [ 0.0118,  0.2402,  0.0249,  ..., -0.1297, -0.2885, -0.2076],\n",
      "         [ 0.0116,  0.2408,  0.0257,  ..., -0.1297, -0.2880, -0.2084],\n",
      "         ...,\n",
      "         [ 0.0112,  0.2406,  0.0254,  ..., -0.1292, -0.2880, -0.2085],\n",
      "         [ 0.0119,  0.2397,  0.0255,  ..., -0.1299, -0.2873, -0.2080],\n",
      "         [ 0.0110,  0.2404,  0.0251,  ..., -0.1298, -0.2878, -0.2092]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0542,  0.2357,  0.0392,  ..., -0.1442, -0.3077, -0.2318],\n",
      "         [-0.0549,  0.2349,  0.0394,  ..., -0.1440, -0.3069, -0.2309],\n",
      "         [-0.0545,  0.2361,  0.0385,  ..., -0.1457, -0.3071, -0.2301],\n",
      "         ...,\n",
      "         [-0.0539,  0.2349,  0.0379,  ..., -0.1442, -0.3077, -0.2309],\n",
      "         [-0.0546,  0.2359,  0.0396,  ..., -0.1441, -0.3074, -0.2313],\n",
      "         [-0.0545,  0.2363,  0.0390,  ..., -0.1446, -0.3075, -0.2300]],\n",
      "\n",
      "        [[ 0.0037,  0.2395,  0.0701,  ..., -0.1685, -0.2866, -0.2353],\n",
      "         [ 0.0047,  0.2394,  0.0690,  ..., -0.1688, -0.2874, -0.2358],\n",
      "         [ 0.0042,  0.2385,  0.0695,  ..., -0.1691, -0.2869, -0.2361],\n",
      "         ...,\n",
      "         [ 0.0051,  0.2396,  0.0697,  ..., -0.1685, -0.2871, -0.2357],\n",
      "         [ 0.0048,  0.2398,  0.0694,  ..., -0.1688, -0.2869, -0.2355],\n",
      "         [ 0.0046,  0.2405,  0.0689,  ..., -0.1687, -0.2868, -0.2344]],\n",
      "\n",
      "        [[-0.0244,  0.1982,  0.0207,  ..., -0.1508, -0.3114, -0.2723],\n",
      "         [-0.0237,  0.1980,  0.0208,  ..., -0.1505, -0.3111, -0.2720],\n",
      "         [-0.0238,  0.1987,  0.0211,  ..., -0.1506, -0.3113, -0.2724],\n",
      "         ...,\n",
      "         [-0.0235,  0.1989,  0.0212,  ..., -0.1509, -0.3116, -0.2726],\n",
      "         [-0.0238,  0.1992,  0.0212,  ..., -0.1519, -0.3106, -0.2722],\n",
      "         [-0.0242,  0.1977,  0.0214,  ..., -0.1512, -0.3120, -0.2722]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([128, 32, 512])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T04:19:11.838763Z",
     "start_time": "2025-11-23T04:19:11.822928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LayNormal (层归一化)\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, esp=1e-12):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.esp = esp\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(-1,keepdim=True)\n",
    "        var = x.var(-1,unbiased=False,keepdim=True)\n",
    "        out = (x-mean)/torch.sqrt(var+self.esp)\n",
    "        out = self.gamma*out+self.beta\n",
    "        return out"
   ],
   "id": "941bc7d932df849f",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T04:19:11.854533Z",
     "start_time": "2025-11-23T04:19:11.838763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 前馈神经网络\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, dropout=0.1):\n",
    "        super(FeedForward,self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "id": "c98330a2ec39379f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T04:19:11.870351Z",
     "start_time": "2025-11-23T04:19:11.854533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 编码层\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, FF_hidden, n_head, dropout=0.1):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model,n_head)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.feedforward = FeedForward(d_model,FF_hidden,dropout)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    def forward(self,x,mask=None):\n",
    "        _x = x\n",
    "        x = self.attention(x,mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x+_x)\n",
    "        _x = x\n",
    "        x = self.feedforward(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x+_x)\n",
    "        return x"
   ],
   "id": "c11bc1d1a8bd5094",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T04:19:11.886288Z",
     "start_time": "2025-11-23T04:19:11.870351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 完整编码器\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, d_model, FF_hidden, n_head, n_layer, device, dropout=0.1):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.embedding = TransformerEmbedding(vocab_size,d_model,max_len,dropout,device)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model,FF_hidden,n_head,dropout)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "    def forward(self,x,s_mask):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,s_mask)\n",
    "        return x"
   ],
   "id": "a3c3c50ecc5d4cfa",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T04:19:11.901727Z",
     "start_time": "2025-11-23T04:19:11.886288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 解码器层\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model,n_head)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(drop_prob)\n",
    "        self.cross_attention = MultiHeadAttention(d_model,n_head)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(drop_prob)\n",
    "        self.ffn = FeedForward(d_model,ffn_hidden,drop_prob)\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(drop_prob)\n",
    "    def forward(self, dec, enc, t_mask, s_mask):\n",
    "        _x = dec\n",
    "        x = self.attention(dec,dec,dec,t_mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x+_x)\n",
    "        _x = x\n",
    "        x = self.cross_attention(x,enc,enc,s_mask) \n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x+_x)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.norm3(x+_x)\n",
    "        return x"
   ],
   "id": "173941f947e14bcf",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T04:19:11.917790Z",
     "start_time": "2025-11-23T04:19:11.901727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 完整解码器\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,dec_voc_size,max_len,d_model,ffn_hidden,n_head,n_layer,drop_prob,device):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.embedding = TransformerEmbedding(dec_voc_size,d_model,max_len, drop_prob, device)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model,ffn_hidden,n_head,drop_prob)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d_model,dec_voc_size)\n",
    "    def forward(self,dec,enc,t_mask,s_mask):\n",
    "        dec=self.embedding(dec)\n",
    "        for layer in self.layers:\n",
    "            dec = layer(dec,enc,t_mask,s_mask)\n",
    "        dec = self.fc(dec)\n",
    "        return dec"
   ],
   "id": "7e9c02c223776b40",
   "outputs": [],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
